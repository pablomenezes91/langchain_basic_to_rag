{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c95063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports para os exercícios\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableBranch, RunnableLambda, RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory, BaseChatMessageHistory\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Carregando arquivos de configuração\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b95b0",
   "metadata": {},
   "source": [
    "# Desafios de Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf15dd2",
   "metadata": {},
   "source": [
    "## Desafio 1\n",
    "Crie uma chain que recebe o nome de um país e retorna 3 curiosidades sobre ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_template = ChatPromptTemplate.from_template(\"Cite 3 curiosidades sobre {country}\")\n",
    "country_model = ChatOpenAI(model='gpt-4o-mini')\n",
    "country_parser = StrOutputParser()\n",
    "\n",
    "country_chain = country_template | country_model | country_parser\n",
    "country_chain.invoke({\"country\":\"Austrália\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4ee8c",
   "metadata": {},
   "source": [
    "## Desafio 2\n",
    "\n",
    "Crie duas chains: a primeira gera uma receita a partir de uma lista de ingredientes, a segunda avalia a receita e sugere melhorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef62737",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_template = ChatPromptTemplate.from_template(\"Me sugira uma receita com os seguintes ingredientes: {ingredientes}\")\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "parser = StrOutputParser()\n",
    "\n",
    "recipe_chain = recipe_template | model | parser\n",
    "\n",
    "ingredientes = [\n",
    "    \"Arroz\",\n",
    "    \"Batata\",\n",
    "    \"Carne Moída\"\n",
    "]\n",
    "\n",
    "improve_recipe_template = ChatPromptTemplate.from_template(\"Sugira melhorias para a seguinte receita: {receita}\")\n",
    "\n",
    "improve_recipe_chain = improve_recipe_template | model | parser\n",
    "\n",
    "generate_recipe_chain = recipe_chain | improve_recipe_chain\n",
    "generate_recipe_chain.invoke({\"ingredientes\":ingredientes})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cefc0d",
   "metadata": {},
   "source": [
    "## Desafio 3 \n",
    "\n",
    "Crie uma chain que recebe a descrição de um problema, classifica a categoria (bug, feature request, dúvida) usando structured output com Pydantic, e depois gera uma resposta adequada ao tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b46d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentifyRequest(BaseModel):\n",
    "    \"\"\"\"Recebe uma string de usuário e retorna a categoria\"\"\"\"\"\n",
    "    category: str = Field(description=\"Define a categoria da mensagem do usuário\")\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "parser = StrOutputParser()\n",
    "\n",
    "received_request_template = ChatPromptTemplate.from_template(\"Você receberá uma mensagem de um usuário e deve classificá-la ou como bug ou feature request ou dúvida: {mensagem}\")\n",
    "\n",
    "chat_identify_request = model.with_structured_output(IdentifyRequest)\n",
    "\n",
    "mensagem = \"O botão de logar deveria ser azul\" \n",
    "\n",
    "identify_request_chain = received_request_template | chat_identify_request \n",
    "\n",
    "process_request_template = ChatPromptTemplate.from_template(\"Você recebe a seguinte mensagem do usuário: '{mensagem}'. Essa mensagem foi categorizada como {category}. Forneça uma resposta adequada ao usuário\")\n",
    "\n",
    "process_request_chain = process_request_template | model | parser\n",
    "\n",
    "runnable_identify_request = RunnableParallel(category=identify_request_chain | (lambda x: x.category), mensagem=lambda x: x['mensagem'])\n",
    "\n",
    "request_chain = runnable_identify_request | process_request_chain\n",
    "\n",
    "request_chain.invoke({\"mensagem\":mensagem})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2857c0a",
   "metadata": {},
   "source": [
    "## Desafio 4\n",
    "\n",
    "\n",
    "Expanda o exercício anterior: dependendo da categoria classificada, a chain deve seguir por caminhos diferentes. Bug vai pra um prompt que pede passos de reprodução, feature request vai pra um prompt que pede justificativa de negócio, dúvida vai pra um prompt que responde diretamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir os prompts e chains de acordo com a categorização\n",
    "\n",
    "default_chain = RunnableLambda(lambda _: \"Não entendi.\")\n",
    "\n",
    "bug_prompt_template = ChatPromptTemplate.from_template(\"Leia o bug report e gere uma resposta ao usuário pedindo para explicar como reproduzir o problema: {mensagem}\")\n",
    "bug_chain = bug_prompt_template | model | parser\n",
    "\n",
    "feature_request_template = ChatPromptTemplate.from_template(\"Leia a feature request do usuário e gere uma resposta pedindo uma justificativa ao usuário para a solicitação: {mensagem}\")\n",
    "feature_request_chain = feature_request_template | model | parser\n",
    "\n",
    "question_template = ChatPromptTemplate.from_template(\"Leia a dúvida e procure responder a dúvida do usuário: {mensagem}. Caso não saiba, diga que não sabe mesmo\")\n",
    "question_chain = question_template | model | parser\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: x.get(\"category\") == \"bug\", bug_chain),\n",
    "    (lambda x: x.get(\"category\") == \"feature request\", feature_request_chain),\n",
    "    (lambda x: x.get(\"category\") == \"duvida\", question_chain),\n",
    "    default_chain\n",
    ")\n",
    "\n",
    "new_chain = runnable_identify_request | branch_chain \n",
    "new_chain.invoke({\"mensagem\":mensagem})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d86224",
   "metadata": {},
   "source": [
    "## Desafio 5\n",
    "\n",
    "Crie um chatbot que mantém contexto de conversa. O usuário pode pedir recomendações de filmes, e o bot deve lembrar o que já recomendou e quais preferências o usuário mencionou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d71607",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um cinéfilo que ajuda usuários indicando filmes para os usuários, baseado no que eles pediram\"),\n",
    "    MessagesPlaceholder(variable_name=\"historico\"),\n",
    "    (\"human\",\"{duvida}\")\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "main_chain = chat_prompt_template | model\n",
    "\n",
    "memory = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in memory:\n",
    "        memory[session_id] = InMemoryChatMessageHistory()\n",
    "    return memory[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=main_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"duvida\",\n",
    "    history_messages_key=\"historico\"\n",
    ")\n",
    "\n",
    "complete_chain = chain_with_history | parser\n",
    "complete_chain.invoke({\n",
    "    \"duvida\":\"Me cite 5 filmes de terror\"\n",
    "},{\"configurable\":{\"session_id\":\"user123\"}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e9169",
   "metadata": {},
   "source": [
    "Testando se ele considera o histórico da conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a386b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_chain.invoke({\n",
    "    \"duvida\":\"Qual destes é o mais assustador?\"\n",
    "},{\"configurable\":{\"session_id\":\"user123\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6bf1f",
   "metadata": {},
   "source": [
    "## Desafio 6\n",
    "\n",
    "Crie uma chain que carrega um documento PDF, divide em chunks, armazena num vector store (pode usar FAISS ou Chroma), e responde perguntas sobre o conteúdo do documento. Esse exercício junta embeddings, retriever e chain de QA, e é o padrão mais demandado no mercado pra quem trabalha com LLM em produto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9985267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o PDF\n",
    "faq_loader = PyPDFLoader(\"../arquivos/faq.pdf\")\n",
    "faq = faq_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a309e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividindo em chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "chunk_faq = text_splitter.create_documents(faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando os embeddings\n",
    "\n",
    "vector_faq = FAISS.from_documents(chunk_faq, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f3dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando o retriever\n",
    "\n",
    "faq_retriever = vector_faq.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ee705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para juntar os documentos de chunks \n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_prompt = ChatPromptTemplate.from_template(\"Com base no seguinte contexto: '{contexto}' Responda a pergunta do usuário: {pergunta}\")\n",
    "\n",
    "runnable_find_document = RunnableParallel(\n",
    "    contexto = (lambda x: x['pergunta']) | faq_retriever | format_docs, pergunta = lambda x: x[\"pergunta\"]\n",
    ")\n",
    "\n",
    "faq_chain = runnable_find_document | faq_prompt | model | parser\n",
    "faq_chain.invoke({\"pergunta\":\"O sistema funciona offline?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
